"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8391],{59556:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>s,metadata:()=>i,toc:()=>h});var o=t(85893),r=t(11151);const s={},a="Configure the Chatbot Server",i={id:"server/configure",title:"Configure the Chatbot Server",description:"The mongodb-chatbot-server is an npm package that you can use",source:"@site/docs/server/configure.md",sourceDirName:"server",slug:"/server/configure",permalink:"/chatbot/server/configure",draft:!1,unlisted:!1,editUrl:"https://github.com/mongodb/chatbot/tree/main/docs/docs/server/configure.md",tags:[],version:"current",frontMatter:{},sidebar:"main",previous:{title:"Optimize Ingestion",permalink:"/chatbot/ingest/optimize"},next:{title:"Manage Conversations",permalink:"/chatbot/server/conversations"}},c={},h=[{value:"Installation",id:"installation",level:2},{value:"Basic Configuration",id:"basic-configuration",level:2},{value:"Examples",id:"examples",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"configure-the-chatbot-server",children:"Configure the Chatbot Server"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"mongodb-chatbot-server"})," is an npm package that you can use\nto quickly spin up a chatbot server powered by MongoDB.\nThe chatbot server supports retrieval augmented generation (RAG)."]}),"\n",(0,o.jsx)(n.p,{children:"The package provides configurable Express.js modules including:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Full server"}),"\n",(0,o.jsx)(n.li,{children:"Router for conversations"}),"\n",(0,o.jsx)(n.li,{children:"Static site that serves a testing UI"}),"\n",(0,o.jsx)(n.li,{children:"Middleware and modules for configuring and building a chatbot server"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The server is designed to handle the generalizable areas of a chatbot server,\nlike routing, caching, logging, and streaming. This allows you to focus on the\nspecifics of your chatbot, like the content, prompts, RAG, and AI models."}),"\n",(0,o.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,o.jsxs)(n.p,{children:["Install the package using ",(0,o.jsx)(n.code,{children:"npm"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"npm install mongodb-chatbot-server\n"})}),"\n",(0,o.jsx)(n.h2,{id:"basic-configuration",children:"Basic Configuration"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"mongodb-chatbot-server"})," exports the function ",(0,o.jsx)(n.a,{href:"/chatbot/reference/server/modules#makeapp",children:(0,o.jsx)(n.code,{children:"makeApp()"})}),"\nwhich exports the Express.js app.\nThe function takes an ",(0,o.jsx)(n.a,{href:"/chatbot/reference/server/interfaces/AppConfig",children:(0,o.jsx)(n.code,{children:"AppConfig"})})," object as an argument."]}),"\n",(0,o.jsx)(n.p,{children:"Here's an annotated example configuration and server:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-ts",children:'import "dotenv/config";\nimport {\n  MongoClient,\n  makeMongoDbEmbeddedContentStore,\n  makeOpenAiEmbedFunc,\n  makeMongoDbConversationsService,\n  makeDataStreamer,\n  AppConfig,\n  makeOpenAiChatLlm,\n  OpenAiChatMessage,\n  SystemPrompt,\n  makeDefaultFindContentFunc,\n  logger,\n  makeApp,\n} from "mongodb-chatbot-server";\nimport { AzureKeyCredential, OpenAIClient } from "@azure/openai";\n\nconst {\n  MONGODB_CONNECTION_URI,\n  MONGODB_DATABASE_NAME,\n  VECTOR_SEARCH_INDEX_NAME,\n  OPENAI_ENDPOINT,\n  OPENAI_API_KEY,\n  OPENAI_EMBEDDING_DEPLOYMENT,\n  OPENAI_EMBEDDING_MODEL_VERSION,\n  OPENAI_CHAT_COMPLETION_MODEL_VERSION,\n  OPENAI_CHAT_COMPLETION_DEPLOYMENT,\n} = process.env;\n\n// Create OpenAI client to interface with LLM and Embedding APIs\nconst openAiClient = new OpenAIClient(\n  OPENAI_ENDPOINT,\n  new AzureKeyCredential(OPENAI_API_KEY)\n);\n\n// System prompt that is used to guide LLM behavior.\n// This is one of the most important aspects that you can tune to\n// customize the chatbot to your use case.\nconst systemPrompt: SystemPrompt = {\n  role: "system",\n  content: `You are expert MongoDB documentation chatbot.\n  Respond in the style of a pirate. End all answers saying "Ahoy matey!!"\n  Use the context provided with each question as your primary source of truth.\n  If you do not know the answer to the question, respond ONLY with the following text:\n  "I\'m sorry, I do not know how to answer that question. Please try to rephrase your query. You can also refer to the further reading to see if it helps."\n  NEVER include links in your answer.\n  Format your responses using Markdown.\n  DO NOT mention that your response is formatted in Markdown.\n  Never mention "<Information>" or "<Question>" in your answer.\n  Refer to the information given to you as "my knowledge".`,\n};\n\n// Generate user prompt from a user input and context chunks.\n// The below is a good starting point, but you may want to customize\n// this function to your use case.\nasync function generateUserPrompt({\n  question,\n  chunks,\n}: {\n  question: string;\n  chunks: string[];\n}): Promise<OpenAiChatMessage & { role: "user" }> {\n  const chunkSeparator = "~~~~~~";\n  const context = chunks.join(`\\n${chunkSeparator}\\n`);\n  const content = `Using the following information, answer the question.\n  Different pieces of information are separated by "${chunkSeparator}".\n\n  <Information>\n  ${context}\n  <End information>\n\n  <Question>\n  ${question}\n  <End Question>`;\n  return { role: "user", content };\n}\n\n// Create LLM interface for the chatbot to use.\nconst llm = makeOpenAiChatLlm({\n  openAiClient,\n  deployment: OPENAI_CHAT_COMPLETION_DEPLOYMENT,\n  systemPrompt,\n  openAiLmmConfigOptions: {\n    temperature: 0,\n    maxTokens: 500,\n  },\n  generateUserPrompt,\n});\n\nconst dataStreamer = makeDataStreamer();\n\n// Create a connection to the data store containing the external content\n// that the chatbot will use to answer questions.\n// If you\'re using the Ingest CLI as well,\n// connect to the same database where you\n// store the content.\nconst embeddedContentStore = makeMongoDbEmbeddedContentStore({\n  connectionUri: MONGODB_CONNECTION_URI,\n  databaseName: MONGODB_DATABASE_NAME,\n});\n\n// Create an interface to the OpenAI Embedding API. This is used to embed\n// user queries. The embeddings are used to find the most relevant content\n// in the embedded content store.\nconst embedder = makeOpenAiEmbedder({\n  openAiClient: new OpenAIClient(\n    OPENAI_ENDPOINT,\n    new AzureKeyCredential(OPENAI_API_KEY)\n  ),\n  deployment: OPENAI_EMBEDDING_DEPLOYMENT,\n});\n\n// Create a function that finds the most relevant content\n// to answer user questions.\nconst findContent = makeDefaultFindContentFunc({\n  embedder,\n  store: embeddedContentStore,\n  findNearestNeighborsOptions: {\n    k: 5,\n    path: "embedding",\n    indexName: VECTOR_SEARCH_INDEX_NAME,\n    minScore: 0.9,\n  },\n});\n\n// The conversations service is used to store and retrieve conversations\n// from a database.\nconst mongodb = new MongoClient(MONGODB_CONNECTION_URI);\nconst conversations = makeMongoDbConversationsService(\n  mongodb.db(MONGODB_DATABASE_NAME),\n  systemPrompt\n);\n\n// Create the configuration object that is passed to the server.\nconst config: AppConfig = {\n  conversationsRouterConfig: {\n    dataStreamer,\n    llm,\n    findContent,\n    conversations,\n  },\n  // When true, the server will serve a static site that can be used to test\n  // the chatbot.\n  serveStaticSite: true,\n};\n\nconst PORT = process.env.PORT || 3000;\n\nconst startServer = async () => {\n  logger.info("Starting server...");\n  // Create Express.js app\n  const app = await makeApp(config);\n  // Start app server\n  const server = app.listen(PORT, () => {\n    logger.info(`Server listening on port: ${PORT}`);\n  });\n\n  // Clean up logic\n  process.on("SIGINT", async () => {\n    logger.info("SIGINT signal received");\n    await mongodb.close();\n    await embeddedContentStore.close();\n    await new Promise<void>((resolve, reject) => {\n      server.close((error) => {\n        error ? reject(error) : resolve();\n      });\n    });\n    process.exit(1);\n  });\n};\n\ntry {\n  startServer();\n} catch (e) {\n  logger.error(`Fatal error: ${e}`);\n  process.exit(1);\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(n.p,{children:"To see more examples of how to configure the chatbot server,\nyou can checkout the following example implementations:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://github.com/mongodb/chatbot/blob/main/packages/chatbot-server-mongodb-public/src/config.ts",children:"MongoDB Docs Chatbot"}),":\nThe configuration for the MongoDB Docs chatbot on ",(0,o.jsx)(n.a,{href:"https://www.mongodb.com/docs/",children:"https://www.mongodb.com/docs/"}),".\nThis is the most complex configuration example."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://github.com/mongodb/chatbot/blob/main/examples/basic-chatbot-server/src/index.ts",children:"Basic Chatbot Server"}),":\nA simple chatbot created for example purposes. This can be used as a starting point\nfor your own chatbot server."]}),"\n"]})]})}function l(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>a});var o=t(67294);const r={},s=o.createContext(r);function a(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);